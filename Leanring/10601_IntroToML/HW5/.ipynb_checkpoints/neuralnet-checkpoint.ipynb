{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf18730",
   "metadata": {},
   "source": [
    "### Homework 5 | Frank Yue Ying | yying2@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e8d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af93131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--debug'], dest='debug', nargs=None, const=None, default=False, type=<class 'bool'>, choices=None, help='set to True to show logging', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('train_input', type=str,\n",
    "                    help='path to training input .csv file')\n",
    "parser.add_argument('validation_input', type=str,\n",
    "                    help='path to validation input .csv file')\n",
    "parser.add_argument('train_out', type=str,\n",
    "                    help='path to store prediction on training data')\n",
    "parser.add_argument('validation_out', type=str,\n",
    "                    help='path to store prediction on validation data')\n",
    "parser.add_argument('metrics_out', type=str,\n",
    "                    help='path to store training and testing metrics')\n",
    "parser.add_argument('num_epoch', type=int,\n",
    "                    help='number of training epochs')\n",
    "parser.add_argument('hidden_units', type=int,\n",
    "                    help='number of hidden units')\n",
    "parser.add_argument('init_flag', type=int, choices=[1, 2],\n",
    "                    help='weight initialization functions, 1: random')\n",
    "parser.add_argument('learning_rate', type=float,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--debug', type=bool, default=False,\n",
    "                    help='set to True to show logging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50fb97c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.train_input = \"tiny_train.csv\"\n",
    "parser.validation_input = \"tiny_validation.csv\"\n",
    "parser.train_out = \"tiny_train_out.labels\"\n",
    "parser.validation_out = \"tiny_validation_out.labels\"\n",
    "parser.metrics_out = \"tiny_metrics_out.text\"\n",
    "parser.num_epoch = 2\n",
    "parser.hidden_units = 4\n",
    "parser.init_flag = 2\n",
    "parser.learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f21c091",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]] [[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.38777878e-16 6.93889390e-17 6.93889390e-17 6.93889390e-17\n",
      "  6.93889390e-17]]\n",
      "end of gradient\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]] [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-5.03069808e-16 -2.51534904e-16 -2.51534904e-16 -2.51534904e-16\n",
      "  -2.51534904e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "end of gradient\n",
      "[[-2.91491055e-29  0.00000000e+00 -2.91491055e-29 -2.91491055e-29\n",
      "  -2.91491055e-29  0.00000000e+00]\n",
      " [-2.91491055e-29  0.00000000e+00 -2.91491055e-29 -2.91491055e-29\n",
      "  -2.91491055e-29  0.00000000e+00]\n",
      " [-2.91491055e-29  0.00000000e+00 -2.91491055e-29 -2.91491055e-29\n",
      "  -2.91491055e-29  0.00000000e+00]\n",
      " [-2.91491055e-29  0.00000000e+00 -2.91491055e-29 -2.91491055e-29\n",
      "  -2.91491055e-29  0.00000000e+00]] [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-1.46584134e-14 -7.32920669e-15 -7.32920669e-15 -7.32920669e-15\n",
      "  -7.32920669e-15]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "end of gradient\n",
      "[[-2.84722705e-29  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -2.84722705e-29  0.00000000e+00]\n",
      " [-2.84722705e-29  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -2.84722705e-29  0.00000000e+00]\n",
      " [-2.84722705e-29  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -2.84722705e-29  0.00000000e+00]\n",
      " [-2.84722705e-29  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -2.84722705e-29  0.00000000e+00]] [[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [5.19029264e-14 2.59514632e-14 2.59514632e-14 2.59514632e-14\n",
      "  2.59514632e-14]]\n",
      "end of gradient\n",
      "[[-3.14489169e-25  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -3.14489169e-25  0.00000000e+00]\n",
      " [-3.14489169e-25  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -3.14489169e-25  0.00000000e+00]\n",
      " [-3.14489169e-25  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -3.14489169e-25  0.00000000e+00]\n",
      " [-3.14489169e-25  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -3.14489169e-25  0.00000000e+00]] [[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.52877710e-12 7.64388552e-13 7.64388552e-13 7.64388552e-13\n",
      "  7.64388552e-13]]\n",
      "end of gradient\n",
      "[[-3.25454377e-25  0.00000000e+00 -3.25454377e-25 -3.25454377e-25\n",
      "  -3.25454377e-25  0.00000000e+00]\n",
      " [-3.25454377e-25  0.00000000e+00 -3.25454377e-25 -3.25454377e-25\n",
      "  -3.25454377e-25  0.00000000e+00]\n",
      " [-3.25454377e-25  0.00000000e+00 -3.25454377e-25 -3.25454377e-25\n",
      "  -3.25454377e-25  0.00000000e+00]\n",
      " [-3.25454377e-25  0.00000000e+00 -3.25454377e-25 -3.25454377e-25\n",
      "  -3.25454377e-25  0.00000000e+00]] [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-5.43048245e-12 -2.71524123e-12 -2.71524123e-12 -2.71524123e-12\n",
      "  -2.71524123e-12]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "end of gradient\n",
      "[[-3.8874309e-22  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  -3.8874309e-22  0.0000000e+00]\n",
      " [-3.8874309e-22  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  -3.8874309e-22  0.0000000e+00]\n",
      " [-3.8874309e-22  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  -3.8874309e-22  0.0000000e+00]\n",
      " [-3.8874309e-22  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  -3.8874309e-22  0.0000000e+00]] [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [6.2211486e-11 3.1105743e-11 3.1105743e-11 3.1105743e-11 3.1105743e-11]]\n",
      "end of gradient\n",
      "[[-7.67857129e-21  0.00000000e+00 -7.67857129e-21 -7.67857129e-21\n",
      "  -7.67857129e-21  0.00000000e+00]\n",
      " [-7.67857129e-21  0.00000000e+00 -7.67857129e-21 -7.67857129e-21\n",
      "  -7.67857129e-21  0.00000000e+00]\n",
      " [-7.67857129e-21  0.00000000e+00 -7.67857129e-21 -7.67857129e-21\n",
      "  -7.67857129e-21  0.00000000e+00]\n",
      " [-7.67857129e-21  0.00000000e+00 -7.67857129e-21 -7.67857129e-21\n",
      "  -7.67857129e-21  0.00000000e+00]] [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-3.56714831e-10 -1.78357416e-10 -1.78357416e-10 -1.78357416e-10\n",
      "  -1.78357416e-10]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "end of gradient\n",
      "[[-7.46602127e-19  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -7.46602127e-19  0.00000000e+00]\n",
      " [-7.46602127e-19  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -7.46602127e-19  0.00000000e+00]\n",
      " [-7.46602127e-19  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -7.46602127e-19  0.00000000e+00]\n",
      " [-7.46602127e-19  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -7.46602127e-19  0.00000000e+00]] [[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.96081258e-09 1.48040629e-09 1.48040629e-09 1.48040629e-09\n",
      "  1.48040629e-09]]\n",
      "end of gradient\n",
      "[[-2.84479258e-17  0.00000000e+00 -2.84479258e-17 -2.84479258e-17\n",
      "  -2.84479258e-17  0.00000000e+00]\n",
      " [-2.84479258e-17  0.00000000e+00 -2.84479258e-17 -2.84479258e-17\n",
      "  -2.84479258e-17  0.00000000e+00]\n",
      " [-2.84479258e-17  0.00000000e+00 -2.84479258e-17 -2.84479258e-17\n",
      "  -2.84479258e-17  0.00000000e+00]\n",
      " [-2.84479258e-17  0.00000000e+00 -2.84479258e-17 -2.84479258e-17\n",
      "  -2.84479258e-17  0.00000000e+00]] [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-1.98719068e-08 -9.93595342e-09 -9.93595342e-09 -9.93595342e-09\n",
      "  -9.93595342e-09]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "end of gradient\n",
      "[[-4.68244489e-14  0.00000000e+00 -4.68244489e-14 -4.68244489e-14\n",
      "  -4.68244489e-14  0.00000000e+00]\n",
      " [-4.68244489e-14  0.00000000e+00 -4.68244489e-14 -4.68244489e-14\n",
      "  -4.68244489e-14  0.00000000e+00]\n",
      " [-4.68244489e-14  0.00000000e+00 -4.68244489e-14 -4.68244489e-14\n",
      "  -4.68244489e-14  0.00000000e+00]\n",
      " [-4.68244489e-14  0.00000000e+00 -4.68244489e-14 -4.68244489e-14\n",
      "  -4.68244489e-14  0.00000000e+00]] [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-5.85436063e-07 -2.92718031e-07 -2.92718031e-07 -2.92718031e-07\n",
      "  -2.92718031e-07]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "end of gradient\n",
      "[[-2.39287034e-14  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -2.39287034e-14  0.00000000e+00]\n",
      " [-2.39287034e-14  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -2.39287034e-14  0.00000000e+00]\n",
      " [-2.39287034e-14  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -2.39287034e-14  0.00000000e+00]\n",
      " [-2.39287034e-14  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -2.39287034e-14  0.00000000e+00]] [[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.00143046e-06 1.00071523e-06 1.00071523e-06 1.00071523e-06\n",
      "  1.00071523e-06]]\n",
      "end of gradient\n",
      "[[-4.672012e-10  0.000000e+00  0.000000e+00  0.000000e+00 -4.672012e-10\n",
      "   0.000000e+00]\n",
      " [-4.672012e-10  0.000000e+00  0.000000e+00  0.000000e+00 -4.672012e-10\n",
      "   0.000000e+00]\n",
      " [-4.672012e-10  0.000000e+00  0.000000e+00  0.000000e+00 -4.672012e-10\n",
      "   0.000000e+00]\n",
      " [-4.672012e-10  0.000000e+00  0.000000e+00  0.000000e+00 -4.672012e-10\n",
      "   0.000000e+00]] [[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [5.89654522e-05 2.94827261e-05 2.94827261e-05 2.94827261e-05\n",
      "  2.94827261e-05]]\n",
      "end of gradient\n",
      "[[-5.01590056e-10  0.00000000e+00 -5.01590056e-10 -5.01590056e-10\n",
      "  -5.01590056e-10  0.00000000e+00]\n",
      " [-5.01590056e-10  0.00000000e+00 -5.01590056e-10 -5.01590056e-10\n",
      "  -5.01590056e-10  0.00000000e+00]\n",
      " [-5.01590056e-10  0.00000000e+00 -5.01590056e-10 -5.01590056e-10\n",
      "  -5.01590056e-10  0.00000000e+00]\n",
      " [-5.01590056e-10  0.00000000e+00 -5.01590056e-10 -5.01590056e-10\n",
      "  -5.01590056e-10  0.00000000e+00]] [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [-0.00020951 -0.00010475 -0.00010475 -0.00010475 -0.00010475]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "end of gradient\n",
      "[[-5.09985227e-06  0.00000000e+00 -5.09985227e-06 -5.09985227e-06\n",
      "  -5.09985227e-06  0.00000000e+00]\n",
      " [-5.09985227e-06  0.00000000e+00 -5.09985227e-06 -5.09985227e-06\n",
      "  -5.09985227e-06  0.00000000e+00]\n",
      " [-5.09985227e-06  0.00000000e+00 -5.09985227e-06 -5.09985227e-06\n",
      "  -5.09985227e-06  0.00000000e+00]\n",
      " [-5.09985227e-06  0.00000000e+00 -5.09985227e-06 -5.09985227e-06\n",
      "  -5.09985227e-06  0.00000000e+00]] [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [-0.00614368 -0.00307184 -0.00307184 -0.00307184 -0.00307184]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "end of gradient\n",
      "[[-4.74206759e-06  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -4.74206759e-06  0.00000000e+00]\n",
      " [-4.74206759e-06  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -4.74206759e-06  0.00000000e+00]\n",
      " [-4.74206759e-06  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -4.74206759e-06  0.00000000e+00]\n",
      " [-4.74206759e-06  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -4.74206759e-06  0.00000000e+00]] [[0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.01967707 0.00983695 0.00983695 0.00983695 0.00983695]]\n",
      "end of gradient\n",
      "[[-0.00241748  0.         -0.00241748 -0.00241748 -0.00241748  0.        ]\n",
      " [-0.00241748  0.         -0.00241748 -0.00241748 -0.00241748  0.        ]\n",
      " [-0.00241748  0.         -0.00241748 -0.00241748 -0.00241748  0.        ]\n",
      " [-0.00241748  0.         -0.00241748 -0.00241748 -0.00241748  0.        ]] [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [-0.13250432 -0.06622085 -0.06622085 -0.06622085 -0.06622085]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "end of gradient\n",
      "[[-0.0058425  0.         0.         0.        -0.0058425  0.       ]\n",
      " [-0.0058425  0.         0.         0.        -0.0058425  0.       ]\n",
      " [-0.0058425  0.         0.         0.        -0.0058425  0.       ]\n",
      " [-0.0058425  0.         0.         0.        -0.0058425  0.       ]] [[0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.24392553 0.11452683 0.11452683 0.11452683 0.11452683]]\n",
      "end of gradient\n",
      "[[-0.02085402  0.          0.          0.         -0.02085402  0.        ]\n",
      " [-0.02085402  0.          0.          0.         -0.02085402  0.        ]\n",
      " [-0.02085402  0.          0.          0.         -0.02085402  0.        ]\n",
      " [-0.02085402  0.          0.          0.         -0.02085402  0.        ]] [[0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.4349705  0.18644868 0.18644868 0.18644868 0.18644868]]\n",
      "end of gradient\n",
      "[[-0.00817716  0.         -0.00817716 -0.00817716 -0.00817716  0.        ]\n",
      " [-0.00817716  0.         -0.00817716 -0.00817716 -0.00817716  0.        ]\n",
      " [-0.00817716  0.         -0.00817716 -0.00817716 -0.00817716  0.        ]\n",
      " [-0.00817716  0.         -0.00817716 -0.00817716 -0.00817716  0.        ]] [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [-0.20680467 -0.07334732 -0.07334732 -0.07334732 -0.07334732]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "end of gradient\n"
     ]
    }
   ],
   "source": [
    "# logging.basicConfig(filename=r\"C:\\Users\\yingy\\Desktop\\std.txt\",format='[%(asctime)s] {%(pathname)s:%(funcName)s:%(lineno)04d} %(levelname)s - %(message)s',\n",
    "#                             datefmt=\"%H:%M:%S\",\n",
    "#                             level=logging.DEBUG,filemode='w')\n",
    "\n",
    "logging.basicConfig(format='[%(asctime)s] {%(pathname)s:%(funcName)s:%(lineno)04d} %(levelname)s - %(message)s',\n",
    "                            datefmt=\"%H:%M:%S\",\n",
    "                            level=logging.DEBUG)\n",
    "\n",
    "logging.debug('*** Debugging Mode ***')\n",
    "# initialize training / test data and labels\n",
    "X_tr, y_tr, X_te, y_te, out_tr, out_te, out_metrics,n_epochs, n_hid, init_flag, lr = args2data(parser)\n",
    "# Build model\n",
    "if int(init_flag) == 1:\n",
    "    nn = NN(lr,n_epochs,random_init,X_tr.shape[1],n_hid,10)\n",
    "if int(init_flag) == 2:\n",
    "    nn = NN(lr,n_epochs,zero_init,X_tr.shape[1],n_hid,10)\n",
    "# train model\n",
    "metrics = train(X_tr, y_tr, nn, X_te, y_te,out_metrics,n_epochs)\n",
    "# test model and get predicted labels and errors\n",
    "tran_pred, train_error = test(X_tr, y_tr, nn)\n",
    "test_pred, test_error = test(X_te, y_te, nn)\n",
    "# # write predicted label and error into file\n",
    "# np.savetxt(out_tr, tran_pred, delimiter=\"\\n\",fmt=\"%i\")\n",
    "# np.savetxt(out_te, test_pred, delimiter=\"\\n\",fmt=\"%i\")\n",
    "# with open(out_metrics, 'w') as f_out: \n",
    "#     f_out.write(metrics+\"\\n\")\n",
    "#     f_out.write(\"error(train): \"+str(train_error)+\"\\n\")\n",
    "#     f_out.write(\"error(validation): \"+str(test_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4221519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.27953251,  0.24690801,  0.24690801,  0.24690801,  0.24690801],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.28748007, -0.28087475, -0.28087475, -0.28087475, -0.28087475]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18722b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_tr, y_tr, nn, X_te, y_te,out_metrics,n_epochs):\n",
    "    \"\"\"\n",
    "    Train the network using SGD for some epochs.\n",
    "    :param X_tr: train data\n",
    "    :param y_tr: train label\n",
    "    :param nn: neural network class\n",
    "    \"\"\"\n",
    "    metrics = \"\"\n",
    "    for e in range(n_epochs):\n",
    "        X_tr_shuffle,y_tr_shuffle = shuffle(X_tr,y_tr,e)\n",
    "        for i in range(len(y_tr_shuffle)):\n",
    "            x = X_tr_shuffle[i]\n",
    "            y = y_tr_shuffle[i]\n",
    "            y_hat = forward(x, nn)\n",
    "            g_alpha,g_beta = backward(x, y, y_hat, nn)\n",
    "            print(g_alpha,g_beta)\n",
    "            print(\"end of gradient\")\n",
    "            nn.grad_sum_w1+= np.multiply(g_alpha,g_alpha)\n",
    "            nn.grad_sum_w2+= np.multiply(g_beta,g_beta)\n",
    "            nn.w1 = nn.w1 - ((nn.lr/(nn.grad_sum_w1+nn.epsilon)**0.5))*g_alpha\n",
    "            nn.w2 = nn.w2 - ((nn.lr/(nn.grad_sum_w2+nn.epsilon)**0.5))*g_beta\n",
    "            # nn.w1 = nn.w1 - nn.lr*g_alpha\n",
    "            # nn.w2 = nn.w2 - nn.lr*g_beta\n",
    "        metrics = metrics +\"\\n\"+avg_ce(e,X_tr, y_tr, nn, X_te, y_te,out_metrics)\n",
    "    return metrics.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b69d10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, nn):\n",
    "    \"\"\"\n",
    "    Neural network forward computation.\n",
    "    Follow the pseudocode!\n",
    "    :param X: input data\n",
    "    :param nn: neural network class\n",
    "    :return: output probability\n",
    "    \"\"\"\n",
    "    a = linear(X,nn,1)\n",
    "    z = sigmoid(a)\n",
    "    b = linear(z,nn,2)\n",
    "    y_hat = softmax(b)\n",
    "    print_weights(nn)\n",
    "    return y_hat\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59e52b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_crossentropy(y,y_hat):\n",
    "    y_array = np.zeros([1,10])\n",
    "    y_array[0][y] = 1\n",
    "    return (-1*(y_array/y_hat)*np.dot(y_hat.T,(y_array-y_hat).T)).T\n",
    "# return g_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55a3daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_sigmoid(a,z,g_z):\n",
    "    dz_da = np.exp(a)/(1+np.exp(a))**2\n",
    "    return np.multiply(g_z,dz_da)\n",
    "#return g_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c562375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_linear(x,nn,i,g): # i = 1 for alpha, i = 2 for beta\n",
    "    if int(i) == 1:\n",
    "        return np.dot(g,x.reshape(1,len(x)).T)\n",
    "    if int(i) == 2:\n",
    "        return np.insert(x,0,1).reshape(len(x),1).dot(g.T).T, np.dot(nn.w2[:,1:].T,g).T\n",
    "\n",
    "#return i = 2 --> (g_beta,g_z) or i = 1 --> (g_alpha,g_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72cea92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, y, y_hat, nn):\n",
    "    \"\"\"\n",
    "    Neural network backward computation.\n",
    "    Follow the pseudocode!\n",
    "    :param X: input data\n",
    "    :param y: label\n",
    "    :param y_hat: prediction\n",
    "    :param nn: neural network class\n",
    "    :return:\n",
    "    d_w1: gradients for w1\n",
    "    d_w2: gradients for w2\n",
    "    \"\"\"\n",
    "    a = linear(X,nn,1)\n",
    "    z = sigmoid(a)\n",
    "    b = linear(z,nn,2)\n",
    "    g_b = d_crossentropy(y,y_hat)\n",
    "    g_beta,g_z = d_linear(z,nn,2,g_b)\n",
    "    g_a = d_sigmoid(a,z,g_z)\n",
    "    g_alpha = d_linear(X,nn,1,g_a)\n",
    "    print_weights(nn)\n",
    "    return g_alpha,g_beta\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c92948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X, y, epoch):\n",
    "    \"\"\"\n",
    "    Permute the training data for SGD.\n",
    "    :param X: The original input data in the order of the file.\n",
    "    :param y: The original labels in the order of the file.\n",
    "    :param epoch: The epoch number (0-indexed).\n",
    "    :return: Permuted X and y training data for the epoch.\n",
    "    \"\"\"\n",
    "    np.random.seed(epoch)\n",
    "    N = len(y)\n",
    "    ordering = np.random.permutation(N)\n",
    "    return X[ordering], y[ordering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990bf9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_init(shape):\n",
    "    \"\"\"\n",
    "    Randomly initialize a numpy array of the specified shape\n",
    "    :param shape: list or tuple of shapes\n",
    "    :return: initialized weights\n",
    "    \"\"\"\n",
    "    # DO NOT CHANGE THIS\n",
    "    np.random.seed(np.prod(shape))\n",
    "\n",
    "    # Implement random initialization here\n",
    "    rand_int = np.random.uniform(-0.1, 0.1,shape)\n",
    "    return rand_int\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65b44b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(shape):\n",
    "    \"\"\"\n",
    "    Initialize a numpy array of the specified shape with zero\n",
    "    :param shape: list or tuple of shapes\n",
    "    :return: initialized weights\n",
    "    \"\"\"\n",
    "    return np.zeros(shape)\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b4bcfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def args2data(parser):\n",
    "    \"\"\"\n",
    "    Parse argument, create data and label.\n",
    "    :return:\n",
    "    X_tr: train data (numpy array)\n",
    "    y_tr: train label (numpy array)\n",
    "    X_te: test data (numpy array)\n",
    "    y_te: test label (numpy array)\n",
    "    out_tr: predicted output for train data (file)\n",
    "    out_te: predicted output for test data (file)\n",
    "    out_metrics: output for train and test error (file)\n",
    "    n_epochs: number of train epochs\n",
    "    n_hid: number of hidden units\n",
    "    init_flag: weight initialize flag -- 1 means random, 2 means zero\n",
    "    lr: learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    # # Get data from arguments\n",
    "    out_tr = parser.train_out\n",
    "    out_te = parser.validation_out\n",
    "    out_metrics = parser.metrics_out\n",
    "    n_epochs = parser.num_epoch\n",
    "    n_hid = parser.hidden_units\n",
    "    init_flag = parser.init_flag\n",
    "    lr = parser.learning_rate\n",
    "\n",
    "    X_tr = np.loadtxt(parser.train_input, delimiter=',')\n",
    "    y_tr = X_tr[:, 0].astype(int)\n",
    "    X_tr[:, 0] = 1.0 #add bias terms\n",
    "\n",
    "    X_te = np.loadtxt(parser.validation_input, delimiter=',')\n",
    "    y_te = X_te[:, 0].astype(int)\n",
    "    X_te[:, 0]= 1.0 #add bias terms\n",
    "\n",
    "\n",
    "    return (X_tr, y_tr, X_te, y_te, out_tr, out_te, out_metrics,\n",
    "            n_epochs, n_hid, init_flag, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cb044db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    def __init__(self, lr, n_epoch, weight_init_fn, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialization\n",
    "        :param lr: learning rate\n",
    "        :param n_epoch: number of training epochs\n",
    "        :param weight_init_fn: weight initialization function\n",
    "        :param input_size: number of units in the input layer\n",
    "        :param hidden_size: number of units in the hidden layer\n",
    "        :param output_size: number of units in the output layer\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.n_epoch = n_epoch\n",
    "        self.weight_init_fn = weight_init_fn\n",
    "        self.n_input = input_size\n",
    "        self.n_hidden = hidden_size\n",
    "        self.n_output = output_size\n",
    "\n",
    "        # initialize weights and biases for the models\n",
    "        #HINT: pay attention to bias here\n",
    "        self.w1 = weight_init_fn([hidden_size, input_size]) #alpha\n",
    "        self.w2 = weight_init_fn([output_size, hidden_size+1]) #beta\n",
    "\n",
    "        # initialize parameters for adagrad\n",
    "        self.epsilon = 0.00001\n",
    "        self.grad_sum_w1 = np.zeros([hidden_size, input_size]) #s for alpha\n",
    "        self.grad_sum_w2 = np.zeros([output_size, hidden_size+1]) #s for beta\n",
    "\n",
    "        # feel free to add additional attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd66ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a): #used for activation function of a\n",
    "    e = np.exp(-a)\n",
    "    return e / (1 + e)\n",
    "#return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a19da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(b): #used for activation function of b\n",
    "    e = np.exp(b).sum()\n",
    "    return np.exp(b)/e\n",
    "#return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "448e18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x,nn,i): #used for both (X,alpha) and (z,beta)\n",
    "    if int(i) == 1:\n",
    "        return np.dot(x,nn.w1.T)\n",
    "    if int(i) == 2:\n",
    "        return np.dot(x,nn.w2[:,1:].T)\n",
    "# return a or b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8250d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_ce(e,X_tr, y_tr, nn, X_te, y_te,out_metrics):\n",
    "    y_array_tr = np.zeros([len(y_tr),10])\n",
    "    for i in range(len(y_tr)):\n",
    "        y_array_tr[i][y_tr[i]] = 1\n",
    "    y_array_te = np.zeros([len(y_te),10])\n",
    "    for i in range(len(y_te)):\n",
    "        y_array_te[i][y_te[i]] = 1\n",
    "    ytr_hat = forward(X_tr, nn)\n",
    "    tr_ce = -1.0/len(y_tr)*np.dot(y_array_tr,np.log(ytr_hat).T).sum()\n",
    "    yte_hat = forward(X_te, nn)\n",
    "    te_ce = -1.0/len(y_te)*np.dot(y_array_te,np.log(yte_hat).T).sum()\n",
    "    result = \"epoch=\"+str(e+1)+\" crossentropy(train): \"+str(tr_ce)+\"\\n\"+\"epoch=\"+str(e+1)+\" crossentropy(validation): \"+str(te_ce)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d985c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X, y, nn):\n",
    "    \"\"\"\n",
    "    Compute the label and error rate.\n",
    "    :param X: input data\n",
    "    :param y: label\n",
    "    :param nn: neural network class\n",
    "    :return:\n",
    "    labels: predicted labels\n",
    "    error_rate: prediction error rate\n",
    "    \"\"\"\n",
    "    y_hats = np.empty(len(y), dtype=int)\n",
    "    for i in range(len(y)):\n",
    "        x = X[i]\n",
    "        y_hat = forward(x,nn)\n",
    "        y_hats[i] = np.where(y_hat == np.amax(y_hat))[0]\n",
    "    compare = np.equal(y,y_hats)\n",
    "    error_rate = (float(len(y))-np.count_nonzero(compare))/len(y)\n",
    "    return y_hats,error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7f90aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights(nn):\n",
    "    \"\"\"\n",
    "    An example of how to use logging to print out debugging infos.\n",
    "\n",
    "    Note that we use the debug logging level -- if we use a higher logging\n",
    "    level, we will log things with the default logging configuration,\n",
    "    causing potential slowdowns.\n",
    "\n",
    "    Note that we log NumPy matrices on separate lines -- if we do not do this,\n",
    "    the arrays will be turned into strings even when our logging is set to\n",
    "    ignore debug, causing potential massive slowdowns.\n",
    "    :param nn: your model\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    logging.debug(f\"shape of w1: {nn.w1.shape}\")\n",
    "    logging.debug(nn.w1)\n",
    "    logging.debug(f\"shape of w2: {nn.w2.shape}\")\n",
    "    logging.debug(nn.w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7012fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    import argparse\n",
    "    import logging\n",
    "    import math as m\n",
    "    args = parser.parse_args()\n",
    "    if args.debug:\n",
    "        logging.basicConfig(filename=\"std.txt\",format='[%(asctime)s] {%(pathname)s:%(funcName)s:%(lineno)04d} %(levelname)s - %(message)s',\n",
    "                            datefmt=\"%H:%M:%S\",\n",
    "                            level=logging.DEBUG)\n",
    "\n",
    "    logging.debug('*** Debugging Mode ***')\n",
    "    # Note: You can access arguments like learning rate with args.learning_rate\n",
    "\n",
    "    # initialize training / test data and labels\n",
    "    X_tr, y_tr, X_te, y_te, out_tr, out_te, out_metrics,n_epochs, n_hid, init_flag, lr = args2data(args)\n",
    "    # Build model\n",
    "    if int(init_flag) == 1:\n",
    "        nn = NN(lr,n_epochs,random_init,X_tr.shape[1],n_hid,10)\n",
    "    if int(init_flag) == 2:\n",
    "        nn = NN(lr,n_epochs,zero_init,X_tr.shape[1],n_hid,10)\n",
    "    # train model\n",
    "    metrics = train(X_tr, y_tr, nn, X_te, y_te,out_metrics,n_epochs)\n",
    "    # test model and get predicted labels and errors\n",
    "    tran_pred, train_error = test(X_tr, y_tr, nn)\n",
    "    test_pred, test_error = test(X_te, y_te, nn)\n",
    "    # write predicted label and error into file\n",
    "    np.savetxt(out_tr, tran_pred, delimiter=\"\\n\",fmt=\"%i\")\n",
    "    np.savetxt(out_te, test_pred, delimiter=\"\\n\",fmt=\"%i\")\n",
    "    with open(out_metrics, 'w') as f_out: \n",
    "        f_out.write(metrics+\"\\n\")\n",
    "        f_out.write(\"error(train): \"+str(train_error)+\"\\n\")\n",
    "        f_out.write(\"error(validation): \"+str(test_error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
