# Spring 2022 Introduction to Machine Learning (10601)
***
**Note**: 

This directory contains instructions and personal works for this course. Some content were taken from 

**Course Description**: 

Machine Learning is concerned with computer programs that automatically improve their performance through experience (e.g., programs that learn to recognize human faces, recommend music and movies, and drive autonomous robots). This course covers the theory and practical algorithms for machine learning from a variety of perspectives. We cover topics such as Bayesian networks, decision tree learning, Support Vector Machines, statistical learning methods, unsupervised learning and reinforcement learning. The course covers theoretical concepts such as inductive bias, the PAC learning framework, Bayesian learning methods, margin-based learning, and Occamâ€™s Razor. Programming assignments include hands-on experiments with various learning algorithms. This course is designed to give a graduate-level student a thorough grounding in the methodologies, technologies, mathematics and algorithms currently needed by people who do research in machine learning.

**Course Objective**: 

- Implement and analyze existing learning algorithms, including well-studied methods for classification, regression, structured prediction, clustering, and representation learning
- Integrate multiple facets of practical machine learning in a single system: data preprocessing, learning, regularization and model selection
- Describe the the formal properties of models and algorithms for learning and explain the practical implications of those results
- Compare and contrast different paradigms for learning (supervised, unsupervised, etc.)
- Design experiments to evaluate and compare different machine learning techniques on real-world problems
- Employ probability, statistics, calculus, linear algebra, and optimization in order to develop new predictive models or learning methods
- Given a description of a ML technique, analyze it to identify (1) the expressive power of the formalism; (2) the inductive bias implicit in the algorithm; (3) the size and complexity of the search space; (4) the computational properties of the algorithm: (5) any guarantees (or lack thereof) regarding termination, convergence, correctness, accuracy or generalization power.


***
- Homework 1 (HW1): Decision Stump model with a majority vote
- Homework 2 (HW2): Decision Tree
- Homework 4 (HW4): Logistic Regression
- Homework 5 (HW5): Neural Network
- Homework 7 (HW7): (Bayesian) Hidden Markov Model 
- Homework 8 (HW8): Reinforcement/Q Learning 
