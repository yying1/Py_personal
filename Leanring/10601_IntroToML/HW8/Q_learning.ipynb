{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db105ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from environment import MountainCar, GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb2304",
   "metadata": {},
   "source": [
    "python q_learning.py gw tile gw_simple_weight.out gw_simple_returns.out 1 1 0.0 1 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfa00e4",
   "metadata": {},
   "source": [
    "python q_learning.py gw tile gw_weight.out gw_returns.out 3 5 0.0 0.9 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54ffa74",
   "metadata": {},
   "source": [
    "python q_learning.py mc tile mc_tile_weight.out mc_tile_returns.out 25 200 0.0 0.99 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532cbb9",
   "metadata": {},
   "source": [
    "### Dev Section Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22fcc52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('environment', type=str, choices=['mc', 'gw'],help='the environment to use')\n",
    "parser.add_argument('mode', type=str, choices=['raw', 'tile'],help='mode to run the environment in')\n",
    "parser.add_argument('weight_out', type=str,help='path to output the weights of the linear model')\n",
    "parser.add_argument('returns_out', type=str,help='path to output the returns of the agent')\n",
    "parser.add_argument('episodes', type=int,help='the number of episodes to train the agent for')\n",
    "parser.add_argument('max_iterations', type=int,help='the maximum of the length of an episode')\n",
    "parser.add_argument('epsilon', type=float,help='the value of epsilon for epsilon-greedy')\n",
    "parser.add_argument('gamma', type=float,help='the discount factor gamma')\n",
    "parser.add_argument('learning_rate', type=float,help='the learning rate alpha')\n",
    "parser.add_argument('--debug', type=bool, default=True,help='set to True to show logging')\n",
    "parser.environment = \"gw\"\n",
    "parser.mode = \"tile\"\n",
    "parser.weight_out = \"gw_simple_weight.out\"\n",
    "parser.returns_out = \"gw_simple_returns.out\"\n",
    "parser.episodes = 1\n",
    "parser.max_iterations = 1\n",
    "parser.epsilon = 0.0\n",
    "parser.gamma = 1.0\n",
    "parser.learning_rate = 1.0\n",
    "parser.debug = True\n",
    "args = parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "278336b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:11:50] {C:\\Users\\yingy\\Desktop\\Py_personal\\Leanring\\10601_IntroToML\\HW8\\environment.py:reset:0316} DEBUG - Reset: {10: 1}\n"
     ]
    }
   ],
   "source": [
    "mode = args.mode\n",
    "weight_out = args.weight_out\n",
    "returns_out = args.returns_out\n",
    "episodes = args.episodes\n",
    "max_iterations = args.max_iterations\n",
    "epsilon = args.epsilon\n",
    "gamma = args.gamma\n",
    "learning_rate = args.learning_rate\n",
    "debug = args.debug\n",
    "if args.environment == 'mc':\n",
    "    env = MountainCar(mode=mode, debug=debug)\n",
    "else:\n",
    "    env = GridWorld(mode=mode, debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d22a379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:53:51] {C:\\Users\\yingy\\Desktop\\Py_personal\\Leanring\\10601_IntroToML\\HW8\\environment.py:reset:0316} DEBUG - Reset: {10: 1}\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cf28762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:18:24] {C:\\Users\\yingy\\Desktop\\Py_personal\\Leanring\\10601_IntroToML\\HW8\\environment.py:step:0373} DEBUG - Step (action up): state {3: 1}, reward 50, done True\n"
     ]
    }
   ],
   "source": [
    "reward = env.step(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365198b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize your weights/bias here\n",
    "weights = np.zeros((env.action_space,env.state_space),dtype=float) # Our shape is |A| x |S|, if this helps.\n",
    "bias = 0.0\n",
    "# If you decide to fold in the bias (hint: don't), recall how the bias is defined!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36133c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = []  # This is where you will save the return after each episode\n",
    "for episode in range(episodes):\n",
    "    # Reset the environment at the start of each episode\n",
    "    state = env.reset()  # `state` now is the initial state\n",
    "    for it in range(max_iterations):\n",
    "        explore_action =  np.random.randint(low=0, high=env.action_space - 1, size=(1))\n",
    "        #choose the action that maximizes q(s,a|ω)=sTωa+b\n",
    "        reward = env.step(0)[1]\n",
    "        exploit_action = 0\n",
    "        for action in range(1,env.action_space - 1):\n",
    "            action_reward = \n",
    "            if env.step(action)[1] > reward:\n",
    "                \n",
    "        next_state = env.step()\n",
    "        # TODO: Fill in what we have to do every iteration\n",
    "        # Hint 1: `env.step(ACTION)` makes the agent take an action\n",
    "        #         corresponding to `ACTION` (MUST be an INTEGER)\n",
    "        # Hint 2: The size of the action space is `env.action_space`, and\n",
    "        #         the size of the state space is `env.state_space`\n",
    "        # Hint 3: `ACTION` should be one of 0, 1, ..., env.action_space - 1\n",
    "        # Hint 4: For Grid World, the action mapping is\n",
    "        #         {\"up\": 0, \"down\": 1, \"left\": 2, \"right\": 3}\n",
    "        #         Remember when you call `env.step()` you have to pass\n",
    "        #         the INTEGER representing each action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3a428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ebdc622",
   "metadata": {},
   "source": [
    "### Dev Section End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # Command line inputs\n",
    "    mode = args.mode\n",
    "    weight_out = args.weight_out\n",
    "    returns_out = args.returns_out\n",
    "    episodes = args.episodes\n",
    "    max_iterations = args.max_iterations\n",
    "    epsilon = args.epsilon\n",
    "    gamma = args.gamma\n",
    "    learning_rate = args.learning_rate\n",
    "    debug = args.debug\n",
    "\n",
    "    # We will initialize the environment for you:\n",
    "    if args.environment == 'mc':\n",
    "        env = MountainCar(mode=mode, debug=debug)\n",
    "    else:\n",
    "        env = GridWorld(mode=mode, debug=debug)\n",
    "\n",
    "    # TODO: Initialize your weights/bias here\n",
    "    # weights = ...  # Our shape is |A| x |S|, if this helps.\n",
    "    # bias = ...\n",
    "    # If you decide to fold in the bias (hint: don't), recall how the bias is\n",
    "    # defined!\n",
    "\n",
    "    returns = []  # This is where you will save the return after each episode\n",
    "    for episode in range(episodes):\n",
    "        # Reset the environment at the start of each episode\n",
    "        state = env.reset()  # `state` now is the initial state\n",
    "        for it in range(max_iterations):\n",
    "            # TODO: Fill in what we have to do every iteration\n",
    "            # Hint 1: `env.step(ACTION)` makes the agent take an action\n",
    "            #         corresponding to `ACTION` (MUST be an INTEGER)\n",
    "            # Hint 2: The size of the action space is `env.action_space`, and\n",
    "            #         the size of the state space is `env.state_space`\n",
    "            # Hint 3: `ACTION` should be one of 0, 1, ..., env.action_space - 1\n",
    "            # Hint 4: For Grid World, the action mapping is\n",
    "            #         {\"up\": 0, \"down\": 1, \"left\": 2, \"right\": 3}\n",
    "            #         Remember when you call `env.step()` you have to pass\n",
    "            #         the INTEGER representing each action!\n",
    "            pass  # You can delete this `pass`\n",
    "\n",
    "    # TODO: Save output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a107ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # No need to change anything here\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('environment', type=str, choices=['mc', 'gw'],\n",
    "                        help='the environment to use')\n",
    "    parser.add_argument('mode', type=str, choices=['raw', 'tile'],\n",
    "                        help='mode to run the environment in')\n",
    "    parser.add_argument('weight_out', type=str,\n",
    "                        help='path to output the weights of the linear model')\n",
    "    parser.add_argument('returns_out', type=str,\n",
    "                        help='path to output the returns of the agent')\n",
    "    parser.add_argument('episodes', type=int,\n",
    "                        help='the number of episodes to train the agent for')\n",
    "    parser.add_argument('max_iterations', type=int,\n",
    "                        help='the maximum of the length of an episode')\n",
    "    parser.add_argument('epsilon', type=float,\n",
    "                        help='the value of epsilon for epsilon-greedy')\n",
    "    parser.add_argument('gamma', type=float,\n",
    "                        help='the discount factor gamma')\n",
    "    parser.add_argument('learning_rate', type=float,\n",
    "                        help='the learning rate alpha')\n",
    "    parser.add_argument('--debug', type=bool, default=False,\n",
    "                        help='set to True to show logging')\n",
    "    main(parser.parse_args())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
