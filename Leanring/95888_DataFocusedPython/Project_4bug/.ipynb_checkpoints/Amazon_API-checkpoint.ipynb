{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://rapidapi.com/logicbuilder/api/amazon-product-reviews-keywords\n",
    "Amazon Product Reviews\n",
    "Ended up using this video: https://www.youtube.com/watch?v=DIT8rwyPEns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    }
   ],
   "source": [
    "#Name: Frank Yue Ying\n",
    "#Date: 2021-11-20\n",
    "## Note: this script extract US customer reviews from the ASIN's review page on Amazon.Input is the URL string. Replace \"B07JGL19WK\" with the target ASIN. A Excel file named \"AmazonReviewResult\" will aslo be produced at the end.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "HEADERS = ({'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\AppleWebKit/537.36 (KHTML, like Gecko) \\Chrome/90.0.4430.212 Safari/537.36','Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "#List variable all_reviews will keep all of the reviews scraped for the product (including all pages)\n",
    "all_reviews= []\n",
    "\n",
    "#get_soup extract the html content from the amazon review site such as https://www.amazon.com/product-reviews/B07JGL19WK/?ie=UTF8&reviewerType=all_reviews&pageNumber=1\n",
    "def get_soup(url,HEADERS):\n",
    "    r = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "#get_reviews analyze the soup html text and identify each review through div tag, then extract keep elements from each review including title,date,rating and the review content as a dictionary. Finally append it to all_reviews\n",
    "def get_reviews(soup,all_reviews):\n",
    "    reviews = soup.find_all(\"div\",{'data-hook':'review'})\n",
    "\n",
    "    try: \n",
    "        for item in reviews: \n",
    "            review = {\n",
    "            \"title\": item.find(\"a\",{'data-hook':'review-title'}).text.strip(),\n",
    "            \"date\": item.find(\"span\",{'data-hook':'review-date'}).text.strip(),\n",
    "            \"rating\": item.find(\"i\",{'data-hook':'review-star-rating'}).text.strip(),\n",
    "            \"review\": item.find(\"span\",{'data-hook':'review-body'}).text.strip()\n",
    "            }\n",
    "            all_reviews.append(review)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#use a range loop to extract first 100 pages of review content, each page should have 10 reviews. Reviews from other countries will be ignored. It will auto-stop at the last page\n",
    "for x in range(1,101):\n",
    "    soup= get_soup(f\"https://www.amazon.com/product-reviews/B00164QTY0/?ie=UTF8&reviewerType=all_reviews&pageNumber={x}\",HEADERS)\n",
    "    get_reviews(soup,all_reviews)\n",
    "    if not soup.find(\"li\",{'class':'a-disabled a-last'}):\n",
    "        pass\n",
    "    else:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame(all_reviews)\n",
    "df.to_excel(\"AmazonReviewResult.xlsx\",index = False)\n",
    "print(len(all_reviews))\n",
    "print(\"------Review Titles:\")\n",
    "print(*[x['title'] for x in all_reviews],sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.apigenius.io/docs/services/product-data/operations/get-amz-create\n",
    "4db2a75e63df4a1fb66f380105e4589b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "\n",
    "headers = {\n",
    "    'ApiGenius_API_Key': '{4db2a75e63df4a1fb66f380105e4589b}',\n",
    "}\n",
    "\n",
    "try:\n",
    "    conn = http.client.HTTPSConnection('api.apigenius.io')\n",
    "    conn.request(\"GET\", \"/amazon/amz-create?asin=B07JGL19WK\", \"{body}\", headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    print(data)\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "headers = {\n",
    "    'ApiGenius_API_Key': '{001a9492504f440faa25c74bf8e3504f}',\n",
    "}\n",
    "\n",
    "url = \"https://api.apigenius.io/amazon/amz-create?asin=B07JGL19WK\"\n",
    "\n",
    "response = requests.get(url,headers= headers)\n",
    "print(response.status_code)\n",
    "if response.status_code == 200:\n",
    "    data = json.loads(response.content.decode('utf-8'))\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
