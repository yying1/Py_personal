{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c41431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from roboflow import Roboflow\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"labelbox.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe1c97b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f55db2f",
   "metadata": {},
   "source": [
    "# - Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b63f41",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a36c0",
   "metadata": {},
   "source": [
    "### --- Load Labelbox Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60223240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import labelbox\n",
    "LB_API_KEY = os.getenv(\"labelbox_soccer\")\n",
    "PROJECT_ID = 'clgmw7co50urt070y3evv6o23'\n",
    "client = labelbox.Client(api_key = LB_API_KEY)\n",
    "project = client.get_project(PROJECT_ID)\n",
    "labels = project.export_v2(params={\n",
    "\"data_row_details\": True,\n",
    "\"metadata\": True,\n",
    "\"attachments\": True,\n",
    "\"project_details\": True,\n",
    "\"performance_details\": True,\n",
    "\"label_details\": True,\n",
    "\"interpolated_frames\": True\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc677c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_json = labels.result\n",
    "# print(\"results: \", export_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ec722",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_json[0]['data_row']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca55a56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "export_json[0]['projects']['clgmw7co50urt070y3evv6o23']['labels'][0]['annotations']['classifications']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc2a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {}\n",
    "for i in export_json:\n",
    "    class_dict[i['data_row']['external_id']]= i['projects']['clgmw7co50urt070y3evv6o23']['labels'][0]['annotations']['classifications'][0]['radio_answer']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b1a525",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'480.jpg': 'Yes',\n",
       " '482.jpg': 'Yes',\n",
       " '484.jpg': 'Yes',\n",
       " '486.jpg': 'Yes',\n",
       " '488.jpg': 'Yes',\n",
       " '490.jpg': 'Yes',\n",
       " '492.jpg': 'Yes',\n",
       " '494.jpg': 'Yes',\n",
       " '496.jpg': 'Yes',\n",
       " '498.jpg': 'Yes',\n",
       " '500.jpg': 'Yes',\n",
       " '502.jpg': 'Yes',\n",
       " '504.jpg': 'Yes',\n",
       " '506.jpg': 'Yes',\n",
       " '508.jpg': 'Yes',\n",
       " '510.jpg': 'Yes',\n",
       " '512.jpg': 'Yes',\n",
       " '514.jpg': 'Yes',\n",
       " '516.jpg': 'Yes',\n",
       " '518.jpg': 'Yes',\n",
       " '520.jpg': 'Yes',\n",
       " '522.jpg': 'No',\n",
       " '524.jpg': 'Yes',\n",
       " '526.jpg': 'Yes',\n",
       " '528.jpg': 'Yes',\n",
       " '530.jpg': 'Yes',\n",
       " '532.jpg': 'Yes',\n",
       " '534.jpg': 'No',\n",
       " '536.jpg': 'No',\n",
       " '538.jpg': 'No',\n",
       " '540.jpg': 'Yes',\n",
       " '542.jpg': 'Yes',\n",
       " '544.jpg': 'Yes',\n",
       " '546.jpg': 'Yes',\n",
       " '548.jpg': 'Yes',\n",
       " '550.jpg': 'Yes',\n",
       " '552.jpg': 'Yes',\n",
       " '554.jpg': 'Yes',\n",
       " '556.jpg': 'Yes',\n",
       " '558.jpg': 'Yes',\n",
       " '560.jpg': 'Yes',\n",
       " '562.jpg': 'Yes',\n",
       " '564.jpg': 'Yes',\n",
       " '566.jpg': 'Yes',\n",
       " '568.jpg': 'No',\n",
       " '570.jpg': 'No',\n",
       " '572.jpg': 'No',\n",
       " '574.jpg': 'Yes',\n",
       " '576.jpg': 'Yes',\n",
       " '578.jpg': 'Yes',\n",
       " '580.jpg': 'Yes',\n",
       " '582.jpg': 'Yes',\n",
       " '584.jpg': 'Yes',\n",
       " '586.jpg': 'Yes',\n",
       " '588.jpg': 'Yes',\n",
       " '590.jpg': 'Yes',\n",
       " '592.jpg': 'Yes',\n",
       " '594.jpg': 'Yes',\n",
       " '596.jpg': 'Yes',\n",
       " '598.jpg': 'Yes',\n",
       " '600.jpg': 'Yes',\n",
       " '602.jpg': 'Yes',\n",
       " '604.jpg': 'No',\n",
       " '606.jpg': 'No',\n",
       " '608.jpg': 'Yes',\n",
       " '610.jpg': 'Yes',\n",
       " '612.jpg': 'Yes',\n",
       " '614.jpg': 'Yes',\n",
       " '616.jpg': 'Yes',\n",
       " '618.jpg': 'Yes',\n",
       " '620.jpg': 'Yes',\n",
       " '622.jpg': 'Yes',\n",
       " '624.jpg': 'Yes',\n",
       " '626.jpg': 'Yes',\n",
       " '628.jpg': 'Yes',\n",
       " '630.jpg': 'Yes',\n",
       " '632.jpg': 'No',\n",
       " '634.jpg': 'No',\n",
       " '636.jpg': 'No',\n",
       " '638.jpg': 'No',\n",
       " '640.jpg': 'No',\n",
       " '642.jpg': 'No',\n",
       " '644.jpg': 'No',\n",
       " '646.jpg': 'No',\n",
       " '648.jpg': 'No',\n",
       " '650.jpg': 'No',\n",
       " '652.jpg': 'No',\n",
       " '654.jpg': 'No',\n",
       " '656.jpg': 'No',\n",
       " '658.jpg': 'No',\n",
       " '660.jpg': 'Yes',\n",
       " '662.jpg': 'Yes',\n",
       " '664.jpg': 'Yes',\n",
       " '666.jpg': 'Yes',\n",
       " '668.jpg': 'Yes',\n",
       " '670.jpg': 'No',\n",
       " '672.jpg': 'No',\n",
       " '674.jpg': 'No',\n",
       " '676.jpg': 'No',\n",
       " '678.jpg': 'No',\n",
       " '680.jpg': 'No',\n",
       " '682.jpg': 'Yes',\n",
       " '684.jpg': 'Yes',\n",
       " '686.jpg': 'Yes',\n",
       " '688.jpg': 'Yes',\n",
       " '690.jpg': 'Yes',\n",
       " '692.jpg': 'No',\n",
       " '694.jpg': 'No',\n",
       " '696.jpg': 'No',\n",
       " '698.jpg': 'No',\n",
       " '700.jpg': 'No',\n",
       " '702.jpg': 'No',\n",
       " '704.jpg': 'No',\n",
       " '706.jpg': 'No',\n",
       " '708.jpg': 'No',\n",
       " '710.jpg': 'No',\n",
       " '712.jpg': 'Yes',\n",
       " '714.jpg': 'Yes',\n",
       " '716.jpg': 'Yes',\n",
       " '718.jpg': 'Yes',\n",
       " '720.jpg': 'No',\n",
       " '722.jpg': 'No',\n",
       " '724.jpg': 'No',\n",
       " '726.jpg': 'No',\n",
       " '728.jpg': 'No',\n",
       " '730.jpg': 'No',\n",
       " '732.jpg': 'No',\n",
       " '734.jpg': 'No',\n",
       " '736.jpg': 'Yes',\n",
       " '738.jpg': 'Yes',\n",
       " '740.jpg': 'Yes',\n",
       " '742.jpg': 'Yes',\n",
       " '744.jpg': 'Yes',\n",
       " '746.jpg': 'Yes',\n",
       " '748.jpg': 'Yes',\n",
       " '750.jpg': 'Yes',\n",
       " '752.jpg': 'Yes',\n",
       " '754.jpg': 'Yes',\n",
       " '756.jpg': 'Yes',\n",
       " '760.jpg': 'Yes',\n",
       " '762.jpg': 'Yes',\n",
       " '764.jpg': 'Yes',\n",
       " '766.jpg': 'Yes',\n",
       " '768.jpg': 'Yes',\n",
       " '770.jpg': 'Yes',\n",
       " '772.jpg': 'Yes',\n",
       " '774.jpg': 'No',\n",
       " '776.jpg': 'Yes',\n",
       " '778.jpg': 'Yes',\n",
       " '784.jpg': 'Yes',\n",
       " '786.jpg': 'No',\n",
       " '788.jpg': 'No',\n",
       " '790.jpg': 'No',\n",
       " '792.jpg': 'No',\n",
       " '794.jpg': 'Yes',\n",
       " '796.jpg': 'Yes',\n",
       " '798.jpg': 'Yes',\n",
       " '800.jpg': 'Yes',\n",
       " '802.jpg': 'No',\n",
       " '804.jpg': 'No',\n",
       " '806.jpg': 'No',\n",
       " '808.jpg': 'No',\n",
       " '816.jpg': 'No',\n",
       " '818.jpg': 'Yes',\n",
       " '820.jpg': 'Yes',\n",
       " '822.jpg': 'Yes',\n",
       " '824.jpg': 'No',\n",
       " '826.jpg': 'No',\n",
       " '828.jpg': 'No',\n",
       " '830.jpg': 'No',\n",
       " '832.jpg': 'No',\n",
       " '834.jpg': 'No',\n",
       " '836.jpg': 'No',\n",
       " '838.jpg': 'No',\n",
       " '840.jpg': 'No',\n",
       " '842.jpg': 'No',\n",
       " '844.jpg': 'No',\n",
       " '846.jpg': 'No',\n",
       " '848.jpg': 'No',\n",
       " '850.jpg': 'No',\n",
       " '852.jpg': 'No',\n",
       " '854.jpg': 'No',\n",
       " '856.jpg': 'No',\n",
       " '858.jpg': 'No',\n",
       " '860.jpg': 'No',\n",
       " '866.jpg': 'Yes',\n",
       " '868.jpg': 'Yes',\n",
       " '870.jpg': 'Yes',\n",
       " '872.jpg': 'Yes',\n",
       " '874.jpg': 'Yes',\n",
       " '876.jpg': 'No',\n",
       " '878.jpg': 'No',\n",
       " '880.jpg': 'No',\n",
       " '882.jpg': 'Yes',\n",
       " '884.jpg': 'Yes',\n",
       " '886.jpg': 'Yes',\n",
       " '888.jpg': 'Yes',\n",
       " '890.jpg': 'Yes',\n",
       " '892.jpg': 'Yes',\n",
       " '894.jpg': 'No',\n",
       " '896.jpg': 'No',\n",
       " '758.jpg': 'No',\n",
       " '780.jpg': 'Yes',\n",
       " '782.jpg': 'Yes',\n",
       " '810.jpg': 'No',\n",
       " '812.jpg': 'No',\n",
       " '814.jpg': 'No',\n",
       " '862.jpg': 'No',\n",
       " '864.jpg': 'Yes',\n",
       " '898.jpg': 'No',\n",
       " '900.jpg': 'No',\n",
       " '902.jpg': 'No',\n",
       " '904.jpg': 'No',\n",
       " '906.jpg': 'Yes',\n",
       " '908.jpg': 'Yes',\n",
       " '910.jpg': 'Yes',\n",
       " '912.jpg': 'No',\n",
       " '914.jpg': 'Yes',\n",
       " '916.jpg': 'Yes',\n",
       " '918.jpg': 'Yes',\n",
       " '920.jpg': 'Yes',\n",
       " '922.jpg': 'Yes',\n",
       " '924.jpg': 'Yes',\n",
       " '926.jpg': 'Yes',\n",
       " '928.jpg': 'Yes',\n",
       " '930.jpg': 'Yes',\n",
       " '932.jpg': 'Yes',\n",
       " '934.jpg': 'Yes',\n",
       " '936.jpg': 'Yes',\n",
       " '938.jpg': 'Yes',\n",
       " '940.jpg': 'Yes',\n",
       " '942.jpg': 'Yes',\n",
       " '944.jpg': 'Yes',\n",
       " '946.jpg': 'Yes',\n",
       " '948.jpg': 'Yes',\n",
       " '950.jpg': 'Yes',\n",
       " '952.jpg': 'Yes',\n",
       " '954.jpg': 'Yes',\n",
       " '956.jpg': 'Yes',\n",
       " '958.jpg': 'Yes',\n",
       " '960.jpg': 'Yes',\n",
       " '962.jpg': 'Yes',\n",
       " '964.jpg': 'No',\n",
       " '966.jpg': 'No',\n",
       " '968.jpg': 'No',\n",
       " '970.jpg': 'No',\n",
       " '972.jpg': 'No',\n",
       " '974.jpg': 'No',\n",
       " '976.jpg': 'No',\n",
       " '978.jpg': 'No',\n",
       " '980.jpg': 'No',\n",
       " '982.jpg': 'Yes',\n",
       " '984.jpg': 'Yes',\n",
       " '986.jpg': 'Yes',\n",
       " '988.jpg': 'Yes',\n",
       " '990.jpg': 'Yes',\n",
       " '992.jpg': 'Yes',\n",
       " '994.jpg': 'Yes',\n",
       " '996.jpg': 'Yes',\n",
       " '998.jpg': 'Yes',\n",
       " '1000.jpg': 'Yes',\n",
       " '1002.jpg': 'Yes',\n",
       " '1004.jpg': 'Yes',\n",
       " '1006.jpg': 'Yes',\n",
       " '1008.jpg': 'Yes',\n",
       " '1010.jpg': 'Yes',\n",
       " '1012.jpg': 'Yes',\n",
       " '1014.jpg': 'Yes',\n",
       " '1016.jpg': 'Yes',\n",
       " '1018.jpg': 'Yes',\n",
       " '1020.jpg': 'Yes',\n",
       " '1022.jpg': 'Yes',\n",
       " '1024.jpg': 'Yes',\n",
       " '1026.jpg': 'Yes',\n",
       " '1028.jpg': 'Yes',\n",
       " '1030.jpg': 'Yes',\n",
       " '1032.jpg': 'Yes',\n",
       " '1034.jpg': 'No',\n",
       " '1036.jpg': 'Yes',\n",
       " '1038.jpg': 'Yes',\n",
       " '1040.jpg': 'Yes',\n",
       " '1042.jpg': 'Yes',\n",
       " '1044.jpg': 'Yes',\n",
       " '1046.jpg': 'No',\n",
       " '1048.jpg': 'No',\n",
       " '1050.jpg': 'Yes',\n",
       " '1052.jpg': 'Yes',\n",
       " '1054.jpg': 'Yes',\n",
       " '1056.jpg': 'Yes',\n",
       " '1058.jpg': 'Yes',\n",
       " '1060.jpg': 'Yes',\n",
       " '1062.jpg': 'Yes',\n",
       " '1064.jpg': 'Yes',\n",
       " '1066.jpg': 'Yes',\n",
       " '1068.jpg': 'Yes',\n",
       " '1070.jpg': 'Yes',\n",
       " '1072.jpg': 'Yes',\n",
       " '1074.jpg': 'Yes',\n",
       " '1076.jpg': 'Yes',\n",
       " '1078.jpg': 'Yes',\n",
       " '1080.jpg': 'No',\n",
       " '1082.jpg': 'Yes'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac411e8e",
   "metadata": {},
   "source": [
    "### --- Load images and classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd7df8c",
   "metadata": {},
   "source": [
    "https://financial-engineering.medium.com/tensorflow-2-0-load-images-to-tensorflow-897b8b067fc2\n",
    "\n",
    "Might not work with tf1: https://notebook.community/tensorflow/docs/site/en/r1/tutorials/load_data/images \n",
    "\n",
    "https://keras.io/api/data_loading/image/\n",
    "https://www.tensorflow.org/tutorials/images/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771a3dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION = r\"C:\\Users\\yingy\\Desktop\\Py_personal\\Learning\\soccer\\tf_object_detection\\Soccer Object detection.v1i.tfrecord_ignore_data\"\n",
    "# test_record_fname = dataset.location + '/test/cells.tfrecord'\n",
    "train_record_fname = LOCATION + '/train/people.tfrecord'\n",
    "raw_dataset = tf.data.TFRecordDataset(train_record_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b43fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dict = {}\n",
    "for i, raw_record in enumerate(raw_dataset.take(1)):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    file_name=example.features.feature['image/filename'].bytes_list.value[0].decode()\n",
    "    image_encoded_data = example.features.feature['image/encoded'].bytes_list.value[0]\n",
    "    train_images_dict[file_name.split(\"_\")[0]+'.jpg'] = image_encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6653232b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['610.jpg']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_images_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67294431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afee09ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_label(image_encoded_data, file_name,class_dict):\n",
    "    label = True if class_dict[file_name] == 'Yes' else False\n",
    "    imageStream = io.BytesIO(image_encoded_data)\n",
    "    img = Image.open(imageStream).convert('RGB')\n",
    "    image_array = np.array(img)\n",
    "    image_tensor = tf.convert_to_tensor(image_array, dtype=tf.float32)\n",
    "    return tf.image.resize(image_tensor, [224,224]), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "032a125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "label_list = []\n",
    "for k,v in train_images_dict.items():\n",
    "    image, label = process_image_label(v,k,class_dict)\n",
    "    image_list.append(image)\n",
    "    label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21994955",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = tf.data.Dataset.from_tensor_slices((image_list,label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "754f85b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (224, 224, 3)\n",
      "Label:  True\n"
     ]
    }
   ],
   "source": [
    "for image, label in labeled_dataset.take(1):\n",
    "    print(\"Image shape: \", image.numpy().shape)\n",
    "    print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca898ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44e8bdd5",
   "metadata": {},
   "source": [
    "### --- Load Roboflow Project with image annotation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72258e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_KEY = os.getenv(\"rb_key\")\n",
    "rf = Roboflow(api_key=RF_KEY)\n",
    "project = rf.workspace(\"personal-y96vs\").project(\"soccer-object-detection-yj7hm\")\n",
    "# dataset = project.version(1).download(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248bf72b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset.location = r\"C:\\Users\\yingy\\Desktop\\Py_personal\\Learning\\soccer\\tf_object_detection\\Soccer Object detection.v1i.tfrecord_ignore_data\"\n",
    "# LOCATION = dataset.location\n",
    "LOCATION = r\"C:\\Users\\yingy\\Desktop\\Py_personal\\Learning\\soccer\\tf_object_detection\\Soccer Object detection.v1i.tfrecord_ignore_data\"\n",
    "print(\"location:\", LOCATION)\n",
    "# CLASSES = sorted(project.classes.keys())\n",
    "# print(\"classes:\", CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef50c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_record_fname = dataset.location + '/test/cells.tfrecord'\n",
    "train_record_fname = LOCATION + '/train/people.tfrecord'\n",
    "label_map_pbtxt_fname = LOCATION + '/train/people_label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(train_record_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_record in raw_dataset.take(1):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "#     print(example.features.feature)\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af320285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(raw_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01d168",
   "metadata": {},
   "source": [
    "# - Plot an image with label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf99faa",
   "metadata": {},
   "source": [
    "---\n",
    "Reference: \n",
    "https://www.kaggle.com/code/mistag/tensorflow-tfrecords-demystified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_example = next(iter(raw_dataset)) \n",
    "img_parsed = tf.train.Example.FromString(img_example.numpy())\n",
    "# only extract features we will actually use\n",
    "xmin=img_parsed.features.feature['image/object/bbox/xmin'].float_list.value[:]\n",
    "xmax=img_parsed.features.feature['image/object/bbox/xmax'].float_list.value[:]\n",
    "ymin=img_parsed.features.feature['image/object/bbox/ymin'].float_list.value[:]\n",
    "ymax=img_parsed.features.feature['image/object/bbox/ymax'].float_list.value[:]\n",
    "file_name=img_parsed.features.feature['image/filename'].bytes_list.value[0].decode()\n",
    "classes=img_parsed.features.feature['image/object/class/text'].bytes_list.value[:]\n",
    "class_label=img_parsed.features.feature['image/object/class/label'].int64_list.value[:]\n",
    "img_encoded=img_parsed.features.feature['image/encoded'].bytes_list.value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f9b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox(img, xmin, ymin, xmax, ymax, width, label, score):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    xres, yres = img.size[0], img.size[1]\n",
    "    box = np.multiply([xmin, ymin, xmax, ymax], [xres, yres, xres, yres]).astype(int).tolist()\n",
    "    txt = \" {}: {}%\" if score >= 0. else \" {}\"\n",
    "    txt = txt.format(label, round(score, 1))\n",
    "    ts = draw.textbbox((len(txt)*6,8), txt)\n",
    "    draw.rectangle(box, width=width)\n",
    "    if len(label) > 0:\n",
    "        if box[1] >= ts[1]+3:\n",
    "            xsmin, ysmin = box[0], box[1]-ts[1]-3\n",
    "            xsmax, ysmax = box[0]+ts[0]+2, box[1]\n",
    "        else:\n",
    "            xsmin, ysmin = box[0], box[3]\n",
    "            xsmax, ysmax = box[0]+ts[0]+2, box[3]+ts[1]+1\n",
    "        draw.rectangle([xsmin, ysmin, xsmax, ysmax],width = 1)\n",
    "        draw.text((xsmin, ysmin), txt, fill='white')\n",
    "\n",
    "def plot_img(img, axes, xmin, ymin, xmax, ymax, classes, class_label):\n",
    "    for i in range(len(xmin)):\n",
    "        bbox(img, xmin[i], ymin[i], xmax[i], ymax[i], 2, classes[i].decode(), -1)\n",
    "    plt.setp(axes, xticks=[], yticks=[])\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c590768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "axes = axes = fig.add_subplot(1, 1, 1)\n",
    "img = Image.open(BytesIO(img_encoded))\n",
    "plot_img(img, axes, xmin, ymin, xmax, ymax, classes, class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "    'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
    "    'image/object/bbox/ymin':  tf.io.VarLenFeature( tf.float32),\n",
    "    'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
    "    'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
    "    'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
    "    'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/filename': tf.io.VarLenFeature(tf.string),\n",
    "    'image/object/class/text': tf.io.VarLenFeature(tf.string),\n",
    "    'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "  # Parse the input tf.train.Example proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "parsed_image_dataset = raw_dataset.map(_parse_image_function)\n",
    "parsed_image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "for image_features in parsed_image_dataset.take(1):\n",
    "    image_raw = image_features['image/encoded'].numpy()\n",
    "    display.display(display.Image(data=image_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, raw_record in enumerate(raw_dataset.take(1)):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    info = {}\n",
    "    info['image/encoded'] = example.features.feature['image/encoded'].bytes_list.value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5424a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "imageStream = io.BytesIO(info['image/encoded'])\n",
    "imageFile = Image.open(imageStream)\n",
    "display(imageFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b4ee7",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896c04de",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/guide/data#consuming_tfrecord_data\n",
    "\n",
    "[How to Train YOLO-NAS on a Custom Dataset](https://blog.roboflow.com/yolo-nas-how-to-train-on-custom-dataset/): [Notebook](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb?ref=blog.roboflow.com#scrollTo=sGBOHyPc11LA)\n",
    "\n",
    "[Read tfrecord dataset for images](https://www.tensorflow.org/tutorials/load_data/tfrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08d10d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
